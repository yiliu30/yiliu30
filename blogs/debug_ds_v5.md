# Debugging Transformers Upgrade: v4.57.3 â†’ v5.0

**Date:** January 24, 2026  
**Author:** [Your Name]  
**Status:** In Progress

## Overview

This document tracks the debugging process for accuracy issues encountered when upgrading transformers from v4.57.3 to v5.0, using PyTorch debug mode to compare layer-by-layer outputs.


- Full model Output Comparison

<table>
<tr>
<th style="width: 50%">v4.57.3</th>
<th style="width: 50%">v5.0.0.dev0</th>
</tr>
<tr>
<td>

```bash
The capital of France is Paris. Paris is the most populous city in France, with a population of over 12 million people in its metropolitan area. The city is located in the north

```

</td>
<td>

```bash
The capital of France is Paris.

Answer 1. Paris is the capital city.

The capital city is ParisParis.

The capital of France is Paris. The capital of France is Paris
```

</td>
</tr>
</table>





- One Layer Debug string Comparison


<table>
<tr>
<th style="width: 50%">v4.57.3</th>
<th style="width: 50%">v5.0.0.dev0</th>
</tr>
<tr>
<td>

```bash
    aten::embedding(t$0: bf16[129280, 7168], t$1: i64[1, 6], 128815)  ->  t$2: bf16[1, 6, 7168]  # {'input_hash': ((27375526.173154227, 16171.0, None), {}), 'hash': 788.8026814290788}
    aten::arange.start(0, 6, device=cpu, pin_memory=False)  ->  t$3: i64[6]  # {'hash': 15.0}
    aten::unsqueeze(t$3: i64[6], 0)  ->  t$4: i64[1, 6]  # {'input_hash': ((15.0, None), {}), 'hash': 15.0}
    aten::unsqueeze(t$5: f32[32], 0)  ->  t$6: f32[1, 32]  # {'input_hash': ((3.9489362656597677, None), {}), 'hash': 3.9489362656597677}
    aten::unsqueeze(t$6: f32[1, 32], 2)  ->  t$7: f32[1, 32, 1]  # {'input_hash': ((3.9489362656597677, None), {}), 'hash': 3.9489362656597677}
    aten::expand(t$7: f32[1, 32, 1], [1, -1, 1])  ->  t$8: f32[1, 32, 1]  # {'input_hash': ((3.9489362656597677, [None, None, None]), {}), 'hash': 3.9489362656597677}
    aten::unsqueeze(t$4: i64[1, 6], 1)  ->  t$9: i64[1, 1, 6]  # {'input_hash': ((15.0, None), {}), 'hash': 15.0}
    aten::_to_copy(t$9: i64[1, 1, 6], dtype=torch.float32)  ->  t$10: f32[1, 1, 6]  # {'input_hash': ((15.0,), {'dtype': None}), 'hash': 15.0}
    aten::expand(t$8: f32[1, 32, 1], [1, 32, 1])  ->  t$11: f32[1, 32, 1]  # {'input_hash': ((3.9489362656597677, [None, None, None]), {}), 'hash': 3.9489362656597677}
    aten::view(t$11: f32[1, 32, 1], [1, 32, 1])  ->  t$12: f32[1, 32, 1]  # {'input_hash': ((3.9489362656597677, [None, None, None]), {}), 'hash': 3.9489362656597677}
    aten::expand(t$10: f32[1, 1, 6], [1, 1, 6])  ->  t$13: f32[1, 1, 6]  # {'input_hash': ((15.0, [None, None, None]), {}), 'hash': 15.0}
    aten::view(t$13: f32[1, 1, 6], [1, 1, 6])  ->  t$14: f32[1, 1, 6]  # {'input_hash': ((15.0, [None, None, None]), {}), 'hash': 15.0}
    aten::bmm(t$12: f32[1, 32, 1], t$14: f32[1, 1, 6])  ->  t$15: f32[1, 32, 6]  # {'input_hash': ((3.9489362656597677, 15.0), {}), 'hash': 59.23404385846038}
    aten::_unsafe_view(t$15: f32[1, 32, 6], [1, 32, 6])  ->  t$16: f32[1, 32, 6]  # {'input_hash': ((59.23404385846038, [None, None, None]), {}), 'hash': 59.23404385846038}
    aten::transpose.int(t$16: f32[1, 32, 6], 1, 2)  ->  t$17: f32[1, 6, 32]  # {'input_hash': ((59.23404385846038, None, None), {}), 'hash': 59.23404385846038}
    aten::cat(['t$17: f32[1, 6, 32]', 't$17: f32[1, 6, 32]'], -1)  ->  t$18: f32[1, 6, 64]  # {'input_hash': (([59.23404385846038, 59.23404385846038], None), {}), 'hash': 118.46808771692076}
    aten::cos(t$18: f32[1, 6, 64])  ->  t$19: f32[1, 6, 64]  # {'input_hash': ((118.46808771692076,), {}), 'hash': 355.8641229439527}
    aten::mul.Tensor(t$19: f32[1, 6, 64], 1.0)  ->  t$20: f32[1, 6, 64]  # {'input_hash': ((355.8641229439527, None), {}), 'hash': 355.8641229439527}
    aten::sin(t$18: f32[1, 6, 64])  ->  t$21: f32[1, 6, 64]  # {'input_hash': ((118.46808771692076,), {}), 'hash': 61.185383417837784}
    aten::mul.Tensor(t$21: f32[1, 6, 64], 1.0)  ->  t$22: f32[1, 6, 64]  # {'input_hash': ((61.185383417837784, None), {}), 'hash': 61.185383417837784}
    aten::_to_copy(t$20: f32[1, 6, 64], dtype=torch.bfloat16)  ->  t$23: bf16[1, 6, 64]  # {'input_hash': ((355.8641229439527,), {'dtype': None}), 'hash': 355.8790283203125}
    aten::_to_copy(t$22: f32[1, 6, 64], dtype=torch.bfloat16)  ->  t$24: bf16[1, 6, 64]  # {'input_hash': ((61.185383417837784,), {'dtype': None}), 'hash': 61.14249849319458}
    aten::_to_copy(t$2: bf16[1, 6, 7168], dtype=torch.float32)  ->  t$25: f32[1, 6, 7168]  # {'input_hash': ((788.8026814290788,), {'dtype': None}), 'hash': 788.8026814290788}
    aten::pow.Tensor_Scalar(t$25: f32[1, 6, 7168], 2)  ->  t$26: f32[1, 6, 7168]  # {'input_hash': ((788.8026814290788, None), {}), 'hash': 47.63002008403455}
    aten::mean.dim(t$26: f32[1, 6, 7168], [-1], True)  ->  t$27: f32[1, 6, 1]  # {'input_hash': ((47.63002008403455, [None], None), {}), 'hash': 0.006644813169259578}
    aten::add.Tensor(t$27: f32[1, 6, 1], 1e-06)  ->  t$28: f32[1, 6, 1]  # {'input_hash': ((0.006644813169259578, None), {}), 'hash': 0.006650813214946538}
    aten::rsqrt(t$28: f32[1, 6, 1])  ->  t$29: f32[1, 6, 1]  # {'input_hash': ((0.006650813214946538,), {}), 'hash': 185.91046714782715}
    aten::mul.Tensor(t$25: f32[1, 6, 7168], t$29: f32[1, 6, 1])  ->  t$30: f32[1, 6, 7168]  # {'input_hash': ((788.8026814290788, 185.91046714782715), {}), 'hash': 23589.8728415073}
    aten::_to_copy(t$30: f32[1, 6, 7168], dtype=torch.bfloat16)  ->  t$31: bf16[1, 6, 7168]  # {'input_hash': ((23589.8728415073,), {'dtype': None}), 'hash': 23589.859999522567}
    aten::mul.Tensor(t$32: bf16[7168], t$31: bf16[1, 6, 7168])  ->  t$33: bf16[1, 6, 7168]  # {'input_hash': ((297.11181640625, 23589.859999522567), {}), 'hash': 1156.63240952231}
    aten::t(t$34: bf16[1536, 7168])  ->  t$35: bf16[7168, 1536]  # {'input_hash': ((141606.8712687064,), {}), 'hash': 141606.8712687064}
    aten::view(t$33: bf16[1, 6, 7168], [6, 7168])  ->  t$36: bf16[6, 7168]  # {'input_hash': ((1156.63240952231, [None, None]), {}), 'hash': 1156.63240952231}
    aten::mm(t$36: bf16[6, 7168], t$35: bf16[7168, 1536])  ->  t$37: bf16[6, 1536]  # {'input_hash': ((1156.63240952231, 141606.8712687064), {}), 'hash': 920.8860853910446}
    aten::_unsafe_view(t$37: bf16[6, 1536], [1, 6, 1536])  ->  t$38: bf16[1, 6, 1536]  # {'input_hash': ((920.8860853910446, [None, None, None]), {}), 'hash': 920.8860853910446}
    aten::_to_copy(t$38: bf16[1, 6, 1536], dtype=torch.float32)  ->  t$39: f32[1, 6, 1536]  # {'input_hash': ((920.8860853910446,), {'dtype': None}), 'hash': 920.8860853910446}
    aten::pow.Tensor_Scalar(t$39: f32[1, 6, 1536], 2)  ->  t$40: f32[1, 6, 1536]  # {'input_hash': ((920.8860853910446, None), {}), 'hash': 288.5757960550672}
    aten::mean.dim(t$40: f32[1, 6, 1536], [-1], True)  ->  t$41: f32[1, 6, 1]  # {'input_hash': ((288.5757960550672, [None], None), {}), 'hash': 0.1878748619928956}
    aten::add.Tensor(t$41: f32[1, 6, 1], 1e-06)  ->  t$42: f32[1, 6, 1]  # {'input_hash': ((0.1878748619928956, None), {}), 'hash': 0.18788085784763098}
    aten::rsqrt(t$42: f32[1, 6, 1])  ->  t$43: f32[1, 6, 1]  # {'input_hash': ((0.18788085784763098,), {}), 'hash': 39.29507780075073}
    aten::mul.Tensor(t$39: f32[1, 6, 1536], t$43: f32[1, 6, 1])  ->  t$44: f32[1, 6, 1536]  # {'input_hash': ((920.8860853910446, 39.29507780075073), {}), 'hash': 5729.73890671269}
    aten::_to_copy(t$44: f32[1, 6, 1536], dtype=torch.bfloat16)  ->  t$45: bf16[1, 6, 1536]  # {'input_hash': ((5729.73890671269,), {'dtype': None}), 'hash': 5729.775722026825}
    aten::mul.Tensor(t$46: bf16[1536], t$45: bf16[1, 6, 1536])  ->  t$47: bf16[1, 6, 1536]  # {'input_hash': ((681.26416015625, 5729.775722026825), {}), 'hash': 2617.230959892273}
    aten::t(t$48: bf16[24576, 1536])  ->  t$49: bf16[1536, 24576]  # {'input_hash': ((98730.36440096781,), {}), 'hash': 98730.36440096781}
    aten::view(t$47: bf16[1, 6, 1536], [6, 1536])  ->  t$50: bf16[6, 1536]  # {'input_hash': ((2617.230959892273, [None, None]), {}), 'hash': 2617.230959892273}
    aten::mm(t$50: bf16[6, 1536], t$49: bf16[1536, 24576])  ->  t$51: bf16[6, 24576]  # {'input_hash': ((2617.230959892273, 98730.36440096781), {}), 'hash': 58005.382189182565}
    aten::_unsafe_view(t$51: bf16[6, 24576], [1, 6, 24576])  ->  t$52: bf16[1, 6, 24576]  # {'input_hash': ((58005.382189182565, [None, None, None]), {}), 'hash': 58005.382189182565}
    aten::view(t$52: bf16[1, 6, 24576], [1, 6, -1, 192])  ->  t$53: bf16[1, 6, 128, 192]  # {'input_hash': ((58005.382189182565, [None, None, None, None]), {}), 'hash': 58005.382189182565}
    aten::transpose.int(t$53: bf16[1, 6, 128, 192], 1, 2)  ->  t$54: bf16[1, 128, 6, 192]  # {'input_hash': ((58005.382189182565, None, None), {}), 'hash': 58005.382189182565}
    aten::split_with_sizes(t$54: bf16[1, 128, 6, 192], [128, 64], -1)  ->  ['t$55: bf16[1, 128, 6, 128]', 't$56: bf16[1, 128, 6, 64]']  # {'input_hash': ((58005.382189182565, [None, None], None), {}), 'hash': [3620.7217003386468, 54384.66048884392]}
    aten::t(t$57: bf16[576, 7168])  ->  t$58: bf16[7168, 576]  # {'input_hash': ((60000.702554143965,), {}), 'hash': 60000.702554143965}
    aten::view(t$33: bf16[1, 6, 7168], [6, 7168])  ->  t$59: bf16[6, 7168]  # {'input_hash': ((1156.63240952231, [None, None]), {}), 'hash': 1156.63240952231}
    aten::mm(t$59: bf16[6, 7168], t$58: bf16[7168, 576])  ->  t$60: bf16[6, 576]  # {'input_hash': ((1156.63240952231, 60000.702554143965), {}), 'hash': 925.7674539089203}
    aten::_unsafe_view(t$60: bf16[6, 576], [1, 6, 576])  ->  t$61: bf16[1, 6, 576]  # {'input_hash': ((925.7674539089203, [None, None, None]), {}), 'hash': 925.7674539089203}
    aten::split_with_sizes(t$61: bf16[1, 6, 576], [512, 64], -1)  ->  ['t$62: bf16[1, 6, 512]', 't$63: bf16[1, 6, 64]']  # {'input_hash': ((925.7674539089203, [None, None], None), {}), 'hash': [447.7636544704437, 478.00379943847656]}
    aten::_to_copy(t$62: bf16[1, 6, 512], dtype=torch.float32)  ->  t$64: f32[1, 6, 512]  # {'input_hash': ((447.7636544704437,), {'dtype': None}), 'hash': 447.7636544704437}
    aten::pow.Tensor_Scalar(t$64: f32[1, 6, 512], 2)  ->  t$65: f32[1, 6, 512]  # {'input_hash': ((447.7636544704437, None), {}), 'hash': 2416.9021114688285}
    aten::mean.dim(t$65: f32[1, 6, 512], [-1], True)  ->  t$66: f32[1, 6, 1]  # {'input_hash': ((2416.9021114688285, [None], None), {}), 'hash': 4.7205121368169785}
    aten::add.Tensor(t$66: f32[1, 6, 1], 1e-06)  ->  t$67: f32[1, 6, 1]  # {'input_hash': ((4.7205121368169785, None), {}), 'hash': 4.720518007874489}
    aten::rsqrt(t$67: f32[1, 6, 1])  ->  t$68: f32[1, 6, 1]  # {'input_hash': ((4.720518007874489,), {}), 'hash': 9.387228786945343}
    aten::mul.Tensor(t$64: f32[1, 6, 512], t$68: f32[1, 6, 1])  ->  t$69: f32[1, 6, 512]  # {'input_hash': ((447.7636544704437, 9.387228786945343), {}), 'hash': 681.568125214475}
    aten::_to_copy(t$69: f32[1, 6, 512], dtype=torch.bfloat16)  ->  t$70: bf16[1, 6, 512]  # {'input_hash': ((681.568125214475,), {'dtype': None}), 'hash': 681.4855184555054}
    aten::mul.Tensor(t$71: bf16[512], t$70: bf16[1, 6, 512])  ->  t$72: bf16[1, 6, 512]  # {'input_hash': ((3.804391235113144, 681.4855184555054), {}), 'hash': 5.108524536015466}
    aten::t(t$73: bf16[32768, 512])  ->  t$74: bf16[512, 32768]  # {'input_hash': ((54175.308287066175,), {}), 'hash': 54175.308287066175}
    aten::view(t$72: bf16[1, 6, 512], [6, 512])  ->  t$75: bf16[6, 512]  # {'input_hash': ((5.108524536015466, [None, None]), {}), 'hash': 5.108524536015466}
    aten::mm(t$75: bf16[6, 512], t$74: bf16[512, 32768])  ->  t$76: bf16[6, 32768]  # {'input_hash': ((5.108524536015466, 54175.308287066175), {}), 'hash': 111.4665225475328}
    aten::_unsafe_view(t$76: bf16[6, 32768], [1, 6, 32768])  ->  t$77: bf16[1, 6, 32768]  # {'input_hash': ((111.4665225475328, [None, None, None]), {}), 'hash': 111.4665225475328}
    aten::view(t$77: bf16[1, 6, 32768], [1, 6, -1, 256])  ->  t$78: bf16[1, 6, 128, 256]  # {'input_hash': ((111.4665225475328, [None, None, None, None]), {}), 'hash': 111.4665225475328}
    aten::transpose.int(t$78: bf16[1, 6, 128, 256], 1, 2)  ->  t$79: bf16[1, 128, 6, 256]  # {'input_hash': ((111.4665225475328, None, None), {}), 'hash': 111.4665225475328}
    aten::split_with_sizes(t$79: bf16[1, 128, 6, 256], [128, 128], -1)  ->  ['t$80: bf16[1, 128, 6, 128]', 't$81: bf16[1, 128, 6, 128]']  # {'input_hash': ((111.4665225475328, [None, None], None), {}), 'hash': [26.415770137362415, 85.05075241017039]}
    aten::view(t$63: bf16[1, 6, 64], [1, 1, 6, 64])  ->  t$82: bf16[1, 1, 6, 64]  # {'input_hash': ((478.00379943847656, [None, None, None, None]), {}), 'hash': 478.00379943847656}
    aten::unsqueeze(t$23: bf16[1, 6, 64], 1)  ->  t$83: bf16[1, 1, 6, 64]  # {'input_hash': ((355.8790283203125, None), {}), 'hash': 355.8790283203125}
    aten::unsqueeze(t$24: bf16[1, 6, 64], 1)  ->  t$84: bf16[1, 1, 6, 64]  # {'input_hash': ((61.14249849319458, None), {}), 'hash': 61.14249849319458}
    aten::view(t$56: bf16[1, 128, 6, 64], [1, 128, 6, 32, 2])  ->  t$85: bf16[1, 128, 6, 32, 2]  # {'input_hash': ((54384.66048884392, [None, None, None, None, None]), {}), 'hash': 54384.66048884392}
    aten::transpose.int(t$85: bf16[1, 128, 6, 32, 2], 4, 3)  ->  t$86: bf16[1, 128, 6, 2, 32]  # {'input_hash': ((54384.66048884392, None, None), {}), 'hash': 54384.66048884392}
    aten::clone(t$86: bf16[1, 128, 6, 2, 32], memory_format=torch.contiguous_format)  ->  t$87: bf16[1, 128, 6, 2, 32]  # {'input_hash': ((54384.66048884392,), {'memory_format': None}), 'hash': 54384.66048884392}
    aten::_unsafe_view(t$87: bf16[1, 128, 6, 2, 32], [1, 128, 6, 64])  ->  t$88: bf16[1, 128, 6, 64]  # {'input_hash': ((54384.66048884392, [None, None, None, None]), {}), 'hash': 54384.66048884392}
    aten::view(t$82: bf16[1, 1, 6, 64], [1, 1, 6, 32, 2])  ->  t$89: bf16[1, 1, 6, 32, 2]  # {'input_hash': ((478.00379943847656, [None, None, None, None, None]), {}), 'hash': 478.00379943847656}
    aten::transpose.int(t$89: bf16[1, 1, 6, 32, 2], 4, 3)  ->  t$90: bf16[1, 1, 6, 2, 32]  # {'input_hash': ((478.00379943847656, None, None), {}), 'hash': 478.00379943847656}
    aten::clone(t$90: bf16[1, 1, 6, 2, 32], memory_format=torch.contiguous_format)  ->  t$91: bf16[1, 1, 6, 2, 32]  # {'input_hash': ((478.00379943847656,), {'memory_format': None}), 'hash': 478.00379943847656}
    aten::_unsafe_view(t$91: bf16[1, 1, 6, 2, 32], [1, 1, 6, 64])  ->  t$92: bf16[1, 1, 6, 64]  # {'input_hash': ((478.00379943847656, [None, None, None, None]), {}), 'hash': 478.00379943847656}
    aten::mul.Tensor(t$88: bf16[1, 128, 6, 64], t$83: bf16[1, 1, 6, 64])  ->  t$93: bf16[1, 128, 6, 64]  # {'input_hash': ((54384.66048884392, 355.8790283203125), {}), 'hash': 51562.806232601404}
    aten::slice.Tensor(t$88: bf16[1, 128, 6, 64], 3, 0, 32)  ->  t$94: bf16[1, 128, 6, 32]  # {'input_hash': ((54384.66048884392, None, None, None), {}), 'hash': 28109.381856679916}
    aten::slice.Tensor(t$88: bf16[1, 128, 6, 64], 3, 32, 9223372036854775807)  ->  t$95: bf16[1, 128, 6, 32]  # {'input_hash': ((54384.66048884392, None, None, None), {}), 'hash': 26275.278632164}
    aten::neg(t$95: bf16[1, 128, 6, 32])  ->  t$96: bf16[1, 128, 6, 32]  # {'input_hash': ((26275.278632164,), {}), 'hash': 26275.278632164}
    aten::cat(['t$96: bf16[1, 128, 6, 32]', 't$94: bf16[1, 128, 6, 32]'], -1)  ->  t$97: bf16[1, 128, 6, 64]  # {'input_hash': (([26275.278632164, 28109.381856679916], None), {}), 'hash': 54384.66048884392}
    aten::mul.Tensor(t$97: bf16[1, 128, 6, 64], t$84: bf16[1, 1, 6, 64])  ->  t$98: bf16[1, 128, 6, 64]  # {'input_hash': ((54384.66048884392, 61.14249849319458), {}), 'hash': 6145.598498088773}
    aten::add.Tensor(t$93: bf16[1, 128, 6, 64], t$98: bf16[1, 128, 6, 64])  ->  t$99: bf16[1, 128, 6, 64]  # {'input_hash': ((51562.806232601404, 6145.598498088773), {}), 'hash': 54342.5701687336}
    aten::mul.Tensor(t$92: bf16[1, 1, 6, 64], t$83: bf16[1, 1, 6, 64])  ->  t$100: bf16[1, 1, 6, 64]  # {'input_hash': ((478.00379943847656, 355.8790283203125), {}), 'hash': 469.1174964904785}
    aten::slice.Tensor(t$92: bf16[1, 1, 6, 64], 3, 0, 32)  ->  t$101: bf16[1, 1, 6, 32]  # {'input_hash': ((478.00379943847656, None, None, None), {}), 'hash': 171.16246032714844}
    aten::slice.Tensor(t$92: bf16[1, 1, 6, 64], 3, 32, 9223372036854775807)  ->  t$102: bf16[1, 1, 6, 32]  # {'input_hash': ((478.00379943847656, None, None, None), {}), 'hash': 306.8413391113281}
    aten::neg(t$102: bf16[1, 1, 6, 32])  ->  t$103: bf16[1, 1, 6, 32]  # {'input_hash': ((306.8413391113281,), {}), 'hash': 306.8413391113281}
    aten::cat(['t$103: bf16[1, 1, 6, 32]', 't$101: bf16[1, 1, 6, 32]'], -1)  ->  t$104: bf16[1, 1, 6, 64]  # {'input_hash': (([306.8413391113281, 171.16246032714844], None), {}), 'hash': 478.00379943847656}
    aten::mul.Tensor(t$104: bf16[1, 1, 6, 64], t$84: bf16[1, 1, 6, 64])  ->  t$105: bf16[1, 1, 6, 64]  # {'input_hash': ((478.00379943847656, 61.14249849319458), {}), 'hash': 20.15657148777973}
    aten::add.Tensor(t$100: bf16[1, 1, 6, 64], t$105: bf16[1, 1, 6, 64])  ->  t$106: bf16[1, 1, 6, 64]  # {'input_hash': ((469.1174964904785, 20.15657148777973), {}), 'hash': 478.46177673339844}
    aten::expand(t$106: bf16[1, 1, 6, 64], [1, 128, 6, -1])  ->  t$107: bf16[1, 128, 6, 64]  # {'input_hash': ((478.46177673339844, [None, None, None, None]), {}), 'hash': 61243.107421875}
    aten::cat(['t$55: bf16[1, 128, 6, 128]', 't$99: bf16[1, 128, 6, 64]'], -1)  ->  t$108: bf16[1, 128, 6, 192]  # {'input_hash': (([3620.7217003386468, 54342.5701687336], None), {}), 'hash': 57963.29186907224}
    aten::cat(['t$80: bf16[1, 128, 6, 128]', 't$107: bf16[1, 128, 6, 64]'], -1)  ->  t$109: bf16[1, 128, 6, 192]  # {'input_hash': (([26.415770137362415, 61243.107421875], None), {}), 'hash': 61269.52319201236}
    aten::lift_fresh(t$110: bf16[0])  ->  t$110: bf16[0]  # {'input_hash': ((0.0,), {}), 'hash': 0.0}
    aten::lift_fresh(t$111: bf16[0])  ->  t$111: bf16[0]  # {'input_hash': ((0.0,), {}), 'hash': 0.0}
    aten::cat(['t$110: bf16[0]', 't$109: bf16[1, 128, 6, 192]'], -2)  ->  t$112: bf16[1, 128, 6, 192]  # {'input_hash': (([0.0, 61269.52319201236], None), {}), 'hash': 61269.52319201236}
    aten::cat(['t$111: bf16[0]', 't$81: bf16[1, 128, 6, 128]'], -2)  ->  t$113: bf16[1, 128, 6, 128]  # {'input_hash': (([0.0, 85.05075241017039], None), {}), 'hash': 85.05075241017039}
    aten::_to_copy(t$108: bf16[1, 128, 6, 192], dtype=torch.float32)  ->  t$114: f32[1, 128, 6, 192]  # {'input_hash': ((57963.29186907224,), {'dtype': None}), 'hash': 57963.29186907224}
    aten::_to_copy(t$112: bf16[1, 128, 6, 192], dtype=torch.float32)  ->  t$115: f32[1, 128, 6, 192]  # {'input_hash': ((61269.52319201236,), {'dtype': None}), 'hash': 61269.52319201236}
    aten::_to_copy(t$113: bf16[1, 128, 6, 128], dtype=torch.float32)  ->  t$116: f32[1, 128, 6, 128]  # {'input_hash': ((85.05075241017039,), {'dtype': None}), 'hash': 85.05075241017039}
    aten::mul.Scalar(t$114: f32[1, 128, 6, 192], 0.3677414565436974)  ->  t$117: f32[1, 128, 6, 192]  # {'input_hash': ((57963.29186907224, None), {}), 'hash': 21315.505901487195}
    aten::ones([6, 6], dtype=torch.bool, layout=torch.strided, device=cpu)  ->  t$118: b8[6, 6]  # {'hash': 36.0}
    aten::tril(t$118: b8[6, 6])  ->  t$119: b8[6, 6]  # {'input_hash': ((36.0,), {}), 'hash': 21.0}
    aten::scalar_tensor(-inf, dtype=torch.float32, device=cpu)  ->  t$120: f32[]  # {'hash': inf}
    aten::scalar_tensor(0.0, dtype=torch.float32, layout=torch.strided, device=cpu)  ->  t$121: f32[]  # {'hash': 0.0}
    aten::where.self(t$119: b8[6, 6], t$121: f32[], t$120: f32[])  ->  t$122: f32[6, 6]  # {'input_hash': ((21.0, 0.0, inf), {}), 'hash': inf}
    aten::transpose.int(t$115: f32[1, 128, 6, 192], -2, -1)  ->  t$123: f32[1, 128, 192, 6]  # {'input_hash': ((61269.52319201236, None, None), {}), 'hash': 61269.52319201236}
    aten::mul.Scalar(t$123: f32[1, 128, 192, 6], 0.3677414565436974)  ->  t$124: f32[1, 128, 192, 6]  # {'input_hash': ((61269.52319201236, None), {}), 'hash': 22531.34425791007}
    aten::expand(t$117: f32[1, 128, 6, 192], [1, 128, 6, 192])  ->  t$125: f32[1, 128, 6, 192]  # {'input_hash': ((21315.505901487195, [None, None, None, None]), {}), 'hash': 21315.505901487195}
    aten::view(t$125: f32[1, 128, 6, 192], [128, 6, 192])  ->  t$126: f32[128, 6, 192]  # {'input_hash': ((21315.505901487195, [None, None, None]), {}), 'hash': 21315.505901487195}
    aten::expand(t$124: f32[1, 128, 192, 6], [1, 128, 192, 6])  ->  t$127: f32[1, 128, 192, 6]  # {'input_hash': ((22531.34425791007, [None, None, None, None]), {}), 'hash': 22531.34425791007}
    aten::view(t$127: f32[1, 128, 192, 6], [128, 192, 6])  ->  t$128: f32[128, 192, 6]  # {'input_hash': ((22531.34425791007, [None, None, None]), {}), 'hash': 22531.34425791007}
    aten::bmm(t$126: f32[128, 6, 192], t$128: f32[128, 192, 6])  ->  t$129: f32[128, 6, 6]  # {'input_hash': ((21315.505901487195, 22531.34425791007), {}), 'hash': 22071.485942557454}
    aten::_unsafe_view(t$129: f32[128, 6, 6], [1, 128, 6, 6])  ->  t$130: f32[1, 128, 6, 6]  # {'input_hash': ((22071.485942557454, [None, None, None, None]), {}), 'hash': 22071.485942557454}
    aten::add.Tensor(t$130: f32[1, 128, 6, 6], t$122: f32[6, 6])  ->  t$131: f32[1, 128, 6, 6]  # {'input_hash': ((22071.485942557454, inf), {}), 'hash': inf}
    aten::_safe_softmax(t$131: f32[1, 128, 6, 6], -1)  ->  t$132: f32[1, 128, 6, 6]  # {'input_hash': ((inf, None), {}), 'hash': 768.0000004165083}
    aten::_to_copy(t$132: f32[1, 128, 6, 6], dtype=torch.bfloat16)  ->  t$133: bf16[1, 128, 6, 6]  # {'input_hash': ((768.0000004165083,), {'dtype': None}), 'hash': 768.0411031043295}
    aten::expand(t$132: f32[1, 128, 6, 6], [1, 128, 6, 6])  ->  t$134: f32[1, 128, 6, 6]  # {'input_hash': ((768.0000004165083, [None, None, None, None]), {}), 'hash': 768.0000004165083}
    aten::view(t$134: f32[1, 128, 6, 6], [128, 6, 6])  ->  t$135: f32[128, 6, 6]  # {'input_hash': ((768.0000004165083, [None, None, None]), {}), 'hash': 768.0000004165083}
    aten::expand(t$116: f32[1, 128, 6, 128], [1, 128, 6, 128])  ->  t$136: f32[1, 128, 6, 128]  # {'input_hash': ((85.05075241017039, [None, None, None, None]), {}), 'hash': 85.05075241017039}
    aten::view(t$136: f32[1, 128, 6, 128], [128, 6, 128])  ->  t$137: f32[128, 6, 128]  # {'input_hash': ((85.05075241017039, [None, None, None]), {}), 'hash': 85.05075241017039}
    aten::bmm(t$135: f32[128, 6, 6], t$137: f32[128, 6, 128])  ->  t$138: f32[128, 6, 128]  # {'input_hash': ((768.0000004165083, 85.05075241017039), {}), 'hash': 64.43761555664555}
    aten::_unsafe_view(t$138: f32[128, 6, 128], [1, 128, 6, 128])  ->  t$139: f32[1, 128, 6, 128]  # {'input_hash': ((64.43761555664555, [None, None, None, None]), {}), 'hash': 64.43761555664555}
    aten::_to_copy(t$139: f32[1, 128, 6, 128], dtype=torch.bfloat16)  ->  t$140: bf16[1, 128, 6, 128]  # {'input_hash': ((64.43761555664555,), {'dtype': None}), 'hash': 64.43932099102312}
    aten::transpose.int(t$140: bf16[1, 128, 6, 128], 1, 2)  ->  t$141: bf16[1, 6, 128, 128]  # {'input_hash': ((64.43932099102312, None, None), {}), 'hash': 64.43932099102312}
    aten::clone(t$141: bf16[1, 6, 128, 128], memory_format=torch.contiguous_format)  ->  t$142: bf16[1, 6, 128, 128]  # {'input_hash': ((64.43932099102312,), {'memory_format': None}), 'hash': 64.43932099102312}
    aten::view(t$142: bf16[1, 6, 128, 128], [1, 6, -1])  ->  t$143: bf16[1, 6, 16384]  # {'input_hash': ((64.43932099102312, [None, None, None]), {}), 'hash': 64.43932099102312}
    aten::t(t$144: bf16[7168, 16384])  ->  t$145: bf16[16384, 7168]  # {'input_hash': ((402437.33954404993,), {}), 'hash': 402437.33954404993}
    aten::view(t$143: bf16[1, 6, 16384], [6, 16384])  ->  t$146: bf16[6, 16384]  # {'input_hash': ((64.43932099102312, [None, None]), {}), 'hash': 64.43932099102312}
    aten::mm(t$146: bf16[6, 16384], t$145: bf16[16384, 7168])  ->  t$147: bf16[6, 7168]  # {'input_hash': ((64.43932099102312, 402437.33954404993), {}), 'hash': 240.59403831884265}
    aten::_unsafe_view(t$147: bf16[6, 7168], [1, 6, 7168])  ->  t$148: bf16[1, 6, 7168]  # {'input_hash': ((240.59403831884265, [None, None, None]), {}), 'hash': 240.59403831884265}
    aten::add.Tensor(t$2: bf16[1, 6, 7168], t$148: bf16[1, 6, 7168])  ->  t$149: bf16[1, 6, 7168]  # {'input_hash': ((788.8026814290788, 240.59403831884265), {}), 'hash': 832.5981237888336}
    aten::_to_copy(t$149: bf16[1, 6, 7168], dtype=torch.float32)  ->  t$150: f32[1, 6, 7168]  # {'input_hash': ((832.5981237888336,), {'dtype': None}), 'hash': 832.5981237888336}
    aten::pow.Tensor_Scalar(t$150: f32[1, 6, 7168], 2)  ->  t$151: f32[1, 6, 7168]  # {'input_hash': ((832.5981237888336, None), {}), 'hash': 58.39140548157849}
    aten::mean.dim(t$151: f32[1, 6, 7168], [-1], True)  ->  t$152: f32[1, 6, 1]  # {'input_hash': ((58.39140548157849, [None], None), {}), 'hash': 0.008146122156176716}
    aten::add.Tensor(t$152: f32[1, 6, 1], 1e-06)  ->  t$153: f32[1, 6, 1]  # {'input_hash': ((0.008146122156176716, None), {}), 'hash': 0.008152122201863676}
    aten::rsqrt(t$153: f32[1, 6, 1])  ->  t$154: f32[1, 6, 1]  # {'input_hash': ((0.008152122201863676,), {}), 'hash': 169.3051872253418}
    aten::mul.Tensor(t$150: f32[1, 6, 7168], t$154: f32[1, 6, 1])  ->  t$155: f32[1, 6, 7168]  # {'input_hash': ((832.5981237888336, 169.3051872253418), {}), 'hash': 22537.589959858786}
    aten::_to_copy(t$155: f32[1, 6, 7168], dtype=torch.bfloat16)  ->  t$156: bf16[1, 6, 7168]  # {'input_hash': ((22537.589959858786,), {'dtype': None}), 'hash': 22537.0666513443}
    aten::mul.Tensor(t$157: bf16[7168], t$156: bf16[1, 6, 7168])  ->  t$158: bf16[1, 6, 7168]  # {'input_hash': ((122.01806432008743, 22537.0666513443), {}), 'hash': 605.1124907890335}
    aten::t(t$159: bf16[18432, 7168])  ->  t$160: bf16[7168, 18432]  # {'input_hash': ((101450.95126219967,), {}), 'hash': 101450.95126219967}
    aten::view(t$158: bf16[1, 6, 7168], [6, 7168])  ->  t$161: bf16[6, 7168]  # {'input_hash': ((605.1124907890335, [None, None]), {}), 'hash': 605.1124907890335}
    aten::mm(t$161: bf16[6, 7168], t$160: bf16[7168, 18432])  ->  t$162: bf16[6, 18432]  # {'input_hash': ((605.1124907890335, 101450.95126219967), {}), 'hash': 788120.3197385073}
    aten::_unsafe_view(t$162: bf16[6, 18432], [1, 6, 18432])  ->  t$163: bf16[1, 6, 18432]  # {'input_hash': ((788120.3197385073, [None, None, None]), {}), 'hash': 788120.3197385073}
    aten::silu(t$163: bf16[1, 6, 18432])  ->  t$164: bf16[1, 6, 18432]  # {'input_hash': ((788120.3197385073,), {}), 'hash': 691.2022647857666}
    aten::t(t$165: bf16[18432, 7168])  ->  t$166: bf16[7168, 18432]  # {'input_hash': ((301350.00236657704,), {}), 'hash': 301350.00236657704}
    aten::view(t$158: bf16[1, 6, 7168], [6, 7168])  ->  t$167: bf16[6, 7168]  # {'input_hash': ((605.1124907890335, [None, None]), {}), 'hash': 605.1124907890335}
    aten::mm(t$167: bf16[6, 7168], t$166: bf16[7168, 18432])  ->  t$168: bf16[6, 18432]  # {'input_hash': ((605.1124907890335, 301350.00236657704), {}), 'hash': 2273.497127377428}
    aten::_unsafe_view(t$168: bf16[6, 18432], [1, 6, 18432])  ->  t$169: bf16[1, 6, 18432]  # {'input_hash': ((2273.497127377428, [None, None, None]), {}), 'hash': 2273.497127377428}
    aten::mul.Tensor(t$164: bf16[1, 6, 18432], t$169: bf16[1, 6, 18432])  ->  t$170: bf16[1, 6, 18432]  # {'input_hash': ((691.2022647857666, 2273.497127377428), {}), 'hash': 19.982094686583878}
    aten::t(t$171: bf16[7168, 18432])  ->  t$172: bf16[18432, 7168]  # {'input_hash': ((284068.9295961037,), {}), 'hash': 284068.9295961037}
    aten::view(t$170: bf16[1, 6, 18432], [6, 18432])  ->  t$173: bf16[6, 18432]  # {'input_hash': ((19.982094686583878, [None, None]), {}), 'hash': 19.982094686583878}
    aten::mm(t$173: bf16[6, 18432], t$172: bf16[18432, 7168])  ->  t$174: bf16[6, 7168]  # {'input_hash': ((19.982094686583878, 284068.9295961037), {}), 'hash': 265.6633223230019}
    aten::_unsafe_view(t$174: bf16[6, 7168], [1, 6, 7168])  ->  t$175: bf16[1, 6, 7168]  # {'input_hash': ((265.6633223230019, [None, None, None]), {}), 'hash': 265.6633223230019}
    aten::add.Tensor(t$149: bf16[1, 6, 7168], t$175: bf16[1, 6, 7168])  ->  t$176: bf16[1, 6, 7168]  # {'input_hash': ((832.5981237888336, 265.6633223230019), {}), 'hash': 873.137708440423}
    aten::_to_copy(t$176: bf16[1, 6, 7168], dtype=torch.float32)  ->  t$177: f32[1, 6, 7168]  # {'input_hash': ((873.137708440423,), {'dtype': None}), 'hash': 873.137708440423}
    aten::pow.Tensor_Scalar(t$177: f32[1, 6, 7168], 2)  ->  t$178: f32[1, 6, 7168]  # {'input_hash': ((873.137708440423, None), {}), 'hash': 62.648662651667884}
    aten::mean.dim(t$178: f32[1, 6, 7168], [-1], True)  ->  t$179: f32[1, 6, 1]  # {'input_hash': ((62.648662651667884, [None], None), {}), 'hash': 0.008740047574974597}
    aten::add.Tensor(t$179: f32[1, 6, 1], 1e-06)  ->  t$180: f32[1, 6, 1]  # {'input_hash': ((0.008740047574974597, None), {}), 'hash': 0.008746047620661557}
    aten::rsqrt(t$180: f32[1, 6, 1])  ->  t$181: f32[1, 6, 1]  # {'input_hash': ((0.008746047620661557,), {}), 'hash': 162.54175567626953}
    aten::mul.Tensor(t$177: f32[1, 6, 7168], t$181: f32[1, 6, 1])  ->  t$182: f32[1, 6, 7168]  # {'input_hash': ((873.137708440423, 162.54175567626953), {}), 'hash': 22257.334139684648}
    aten::_to_copy(t$182: f32[1, 6, 7168], dtype=torch.bfloat16)  ->  t$183: bf16[1, 6, 7168]  # {'input_hash': ((22257.334139684648,), {'dtype': None}), 'hash': 22258.00124192238}
    aten::mul.Tensor(t$184: bf16[7168], t$183: bf16[1, 6, 7168])  ->  t$185: bf16[1, 6, 7168]  # {'input_hash': ((2747.7391052246094, 22258.00124192238), {}), 'hash': 8401.77631354332}
    aten::alias(t$185: bf16[1, 6, 7168])  ->  t$186: bf16[1, 6, 7168]  # {'input_hash': ((8401.77631354332,), {}), 'hash': 8401.77631354332}
    aten::t(t$187: bf16[129280, 7168])  ->  t$188: bf16[7168, 129280]  # {'input_hash': ((58439660.51988735,), {}), 'hash': 58439660.51988734}
    aten::view(t$186: bf16[1, 6, 7168], [6, 7168])  ->  t$189: bf16[6, 7168]  # {'input_hash': ((8401.77631354332, [None, None]), {}), 'hash': 8401.77631354332}
    aten::mm(t$189: bf16[6, 7168], t$188: bf16[7168, 129280])  ->  t$190: bf16[6, 129280]  # {'input_hash': ((8401.77631354332, 58439660.51988734), {}), 'hash': 1418407.944872737}
    aten::_unsafe_view(t$190: bf16[6, 129280], [1, 6, 129280])  ->  t$191: bf16[1, 6, 129280]  # {'input_hash': ((1418407.944872737, [None, None, None]), {}), 'hash': 1418407.944872737}
CausalLMOutputWithPast(loss=None, logits=tensor([[[ 9.6875e+00, -4.0938e+00,  8.0469e-01,  ...,  1.0312e+00,
           8.4766e-01,  7.7734e-01],
         [ 8.9355e-02, -1.8750e+00, -1.6113e-01,  ...,  1.0742e-01,
          -5.4932e-03,  1.8433e-02],
         [ 1.2031e+00, -2.3438e+00,  4.1406e-01,  ...,  3.3203e-01,
           3.0859e-01,  4.8828e-01],
         [ 5.1875e+00, -6.0938e+00,  2.0020e-01,  ...,  2.7148e-01,
           3.0859e-01,  1.6895e-01],
         [-1.2109e+00, -2.3750e+00,  5.0000e-01,  ...,  9.6680e-02,
           1.2695e-01,  6.4062e-01],
         [-1.5820e-01,  1.0010e-01,  2.4023e-01,  ...,  1.3281e-01,
           1.3086e-01,  3.8281e-01]]], dtype=torch.bfloat16), past_key_values=DynamicCache(layers=[DynamicLayer]), hidden_states=None, attentions=None)

```

</td>
<td>

```bash
    aten::embedding(t$0: bf16[129280, 7168], t$1: i64[1, 6], 128815)  ->  t$2: bf16[1, 6, 7168]  # {'input_hash': ((27375526.173154227, 16171.0, None), {}), 'hash': 788.8026814290788}
    aten::arange(6, device=cpu, pin_memory=False)  ->  t$3: i64[6]  # {'hash': 15.0}
    aten::add.Tensor(t$3: i64[6], 0)  ->  t$4: i64[6]  # {'input_hash': ((15.0, None), {}), 'hash': 15.0}
    aten::unsqueeze(t$4: i64[6], 0)  ->  t$5: i64[1, 6]  # {'input_hash': ((15.0, None), {}), 'hash': 15.0}
    aten::unsqueeze(t$6: f32[32], 0)  ->  t$7: f32[1, 32]  # {'input_hash': ((7.372678196482765e+31, None), {}), 'hash': 7.372678196482765e+31}
    aten::unsqueeze(t$7: f32[1, 32], 2)  ->  t$8: f32[1, 32, 1]  # {'input_hash': ((7.372678196482765e+31, None), {}), 'hash': 7.372678196482765e+31}
    aten::expand(t$8: f32[1, 32, 1], [1, -1, 1])  ->  t$9: f32[1, 32, 1]  # {'input_hash': ((7.372678196482765e+31, [None, None, None]), {}), 'hash': 7.372678196482765e+31}
    aten::unsqueeze(t$5: i64[1, 6], 1)  ->  t$10: i64[1, 1, 6]  # {'input_hash': ((15.0, None), {}), 'hash': 15.0}
    aten::_to_copy(t$10: i64[1, 1, 6], dtype=torch.float32)  ->  t$11: f32[1, 1, 6]  # {'input_hash': ((15.0,), {'dtype': None}), 'hash': 15.0}
    aten::expand(t$9: f32[1, 32, 1], [1, 32, 1])  ->  t$12: f32[1, 32, 1]  # {'input_hash': ((7.372678196482765e+31, [None, None, None]), {}), 'hash': 7.372678196482765e+31}
    aten::view(t$12: f32[1, 32, 1], [1, 32, 1])  ->  t$13: f32[1, 32, 1]  # {'input_hash': ((7.372678196482765e+31, [None, None, None]), {}), 'hash': 7.372678196482765e+31}
    aten::expand(t$11: f32[1, 1, 6], [1, 1, 6])  ->  t$14: f32[1, 1, 6]  # {'input_hash': ((15.0, [None, None, None]), {}), 'hash': 15.0}
    aten::view(t$14: f32[1, 1, 6], [1, 1, 6])  ->  t$15: f32[1, 1, 6]  # {'input_hash': ((15.0, [None, None, None]), {}), 'hash': 15.0}
    aten::bmm(t$13: f32[1, 32, 1], t$15: f32[1, 1, 6])  ->  t$16: f32[1, 32, 6]  # {'input_hash': ((7.372678196482765e+31, 15.0), {}), 'hash': 1.1059017487963386e+33}
    aten::_unsafe_view(t$16: f32[1, 32, 6], [1, 32, 6])  ->  t$17: f32[1, 32, 6]  # {'input_hash': ((1.1059017487963386e+33, [None, None, None]), {}), 'hash': 1.1059017487963386e+33}
    aten::transpose.int(t$17: f32[1, 32, 6], 1, 2)  ->  t$18: f32[1, 6, 32]  # {'input_hash': ((1.1059017487963386e+33, None, None), {}), 'hash': 1.1059017487963388e+33}
    aten::cat(['t$18: f32[1, 6, 32]', 't$18: f32[1, 6, 32]'], -1)  ->  t$19: f32[1, 6, 64]  # {'input_hash': (([1.1059017487963388e+33, 1.1059017487963388e+33], None), {}), 'hash': 2.2118034975926775e+33}
    aten::cos(t$19: f32[1, 6, 64])  ->  t$20: f32[1, 6, 64]  # {'input_hash': ((2.2118034975926775e+33,), {}), 'hash': 339.7364740315825}
    aten::mul.Tensor(t$20: f32[1, 6, 64], 1.0)  ->  t$21: f32[1, 6, 64]  # {'input_hash': ((339.7364740315825, None), {}), 'hash': 339.7364740315825}
    aten::sin(t$19: f32[1, 6, 64])  ->  t$22: f32[1, 6, 64]  # {'input_hash': ((2.2118034975926775e+33,), {}), 'hash': 76.23392300636053}
    aten::mul.Tensor(t$22: f32[1, 6, 64], 1.0)  ->  t$23: f32[1, 6, 64]  # {'input_hash': ((76.23392300636053, None), {}), 'hash': 76.23392300636053}
    aten::_to_copy(t$21: f32[1, 6, 64], dtype=torch.bfloat16)  ->  t$24: bf16[1, 6, 64]  # {'input_hash': ((339.7364740315825,), {'dtype': None}), 'hash': 339.74072265625}
    aten::_to_copy(t$23: f32[1, 6, 64], dtype=torch.bfloat16)  ->  t$25: bf16[1, 6, 64]  # {'input_hash': ((76.23392300636053,), {'dtype': None}), 'hash': 76.23876954196976}
    aten::_to_copy(t$2: bf16[1, 6, 7168], dtype=torch.float32)  ->  t$26: f32[1, 6, 7168]  # {'input_hash': ((788.8026814290788,), {'dtype': None}), 'hash': 788.8026814290788}
    aten::pow.Tensor_Scalar(t$26: f32[1, 6, 7168], 2)  ->  t$27: f32[1, 6, 7168]  # {'input_hash': ((788.8026814290788, None), {}), 'hash': 47.63002008403455}
    aten::mean.dim(t$27: f32[1, 6, 7168], [-1], True)  ->  t$28: f32[1, 6, 1]  # {'input_hash': ((47.63002008403455, [None], None), {}), 'hash': 0.006644813169259578}
    aten::add.Tensor(t$28: f32[1, 6, 1], 1e-06)  ->  t$29: f32[1, 6, 1]  # {'input_hash': ((0.006644813169259578, None), {}), 'hash': 0.006650813214946538}
    aten::rsqrt(t$29: f32[1, 6, 1])  ->  t$30: f32[1, 6, 1]  # {'input_hash': ((0.006650813214946538,), {}), 'hash': 185.91046714782715}
    aten::mul.Tensor(t$26: f32[1, 6, 7168], t$30: f32[1, 6, 1])  ->  t$31: f32[1, 6, 7168]  # {'input_hash': ((788.8026814290788, 185.91046714782715), {}), 'hash': 23589.8728415073}
    aten::_to_copy(t$31: f32[1, 6, 7168], dtype=torch.bfloat16)  ->  t$32: bf16[1, 6, 7168]  # {'input_hash': ((23589.8728415073,), {'dtype': None}), 'hash': 23589.859999522567}
    aten::mul.Tensor(t$33: bf16[7168], t$32: bf16[1, 6, 7168])  ->  t$34: bf16[1, 6, 7168]  # {'input_hash': ((297.11181640625, 23589.859999522567), {}), 'hash': 1156.63240952231}
    aten::t(t$35: bf16[1536, 7168])  ->  t$36: bf16[7168, 1536]  # {'input_hash': ((141606.8712687064,), {}), 'hash': 141606.8712687064}
    aten::view(t$34: bf16[1, 6, 7168], [6, 7168])  ->  t$37: bf16[6, 7168]  # {'input_hash': ((1156.63240952231, [None, None]), {}), 'hash': 1156.63240952231}
    aten::mm(t$37: bf16[6, 7168], t$36: bf16[7168, 1536])  ->  t$38: bf16[6, 1536]  # {'input_hash': ((1156.63240952231, 141606.8712687064), {}), 'hash': 920.8860853910446}
    aten::_unsafe_view(t$38: bf16[6, 1536], [1, 6, 1536])  ->  t$39: bf16[1, 6, 1536]  # {'input_hash': ((920.8860853910446, [None, None, None]), {}), 'hash': 920.8860853910446}
    aten::_to_copy(t$39: bf16[1, 6, 1536], dtype=torch.float32)  ->  t$40: f32[1, 6, 1536]  # {'input_hash': ((920.8860853910446,), {'dtype': None}), 'hash': 920.8860853910446}
    aten::pow.Tensor_Scalar(t$40: f32[1, 6, 1536], 2)  ->  t$41: f32[1, 6, 1536]  # {'input_hash': ((920.8860853910446, None), {}), 'hash': 288.5757960550672}
    aten::mean.dim(t$41: f32[1, 6, 1536], [-1], True)  ->  t$42: f32[1, 6, 1]  # {'input_hash': ((288.5757960550672, [None], None), {}), 'hash': 0.1878748619928956}
    aten::add.Tensor(t$42: f32[1, 6, 1], 1e-06)  ->  t$43: f32[1, 6, 1]  # {'input_hash': ((0.1878748619928956, None), {}), 'hash': 0.18788085784763098}
    aten::rsqrt(t$43: f32[1, 6, 1])  ->  t$44: f32[1, 6, 1]  # {'input_hash': ((0.18788085784763098,), {}), 'hash': 39.29507780075073}
    aten::mul.Tensor(t$40: f32[1, 6, 1536], t$44: f32[1, 6, 1])  ->  t$45: f32[1, 6, 1536]  # {'input_hash': ((920.8860853910446, 39.29507780075073), {}), 'hash': 5729.73890671269}
    aten::_to_copy(t$45: f32[1, 6, 1536], dtype=torch.bfloat16)  ->  t$46: bf16[1, 6, 1536]  # {'input_hash': ((5729.73890671269,), {'dtype': None}), 'hash': 5729.775722026825}
    aten::mul.Tensor(t$47: bf16[1536], t$46: bf16[1, 6, 1536])  ->  t$48: bf16[1, 6, 1536]  # {'input_hash': ((681.26416015625, 5729.775722026825), {}), 'hash': 2617.230959892273}
    aten::t(t$49: bf16[24576, 1536])  ->  t$50: bf16[1536, 24576]  # {'input_hash': ((98730.36440096781,), {}), 'hash': 98730.36440096781}
    aten::view(t$48: bf16[1, 6, 1536], [6, 1536])  ->  t$51: bf16[6, 1536]  # {'input_hash': ((2617.230959892273, [None, None]), {}), 'hash': 2617.230959892273}
    aten::mm(t$51: bf16[6, 1536], t$50: bf16[1536, 24576])  ->  t$52: bf16[6, 24576]  # {'input_hash': ((2617.230959892273, 98730.36440096781), {}), 'hash': 58005.382189182565}
    aten::_unsafe_view(t$52: bf16[6, 24576], [1, 6, 24576])  ->  t$53: bf16[1, 6, 24576]  # {'input_hash': ((58005.382189182565, [None, None, None]), {}), 'hash': 58005.382189182565}
    aten::view(t$53: bf16[1, 6, 24576], [1, 6, -1, 192])  ->  t$54: bf16[1, 6, 128, 192]  # {'input_hash': ((58005.382189182565, [None, None, None, None]), {}), 'hash': 58005.382189182565}
    aten::transpose.int(t$54: bf16[1, 6, 128, 192], 1, 2)  ->  t$55: bf16[1, 128, 6, 192]  # {'input_hash': ((58005.382189182565, None, None), {}), 'hash': 58005.382189182565}
    aten::split_with_sizes(t$55: bf16[1, 128, 6, 192], [128, 64], -1)  ->  ['t$56: bf16[1, 128, 6, 128]', 't$57: bf16[1, 128, 6, 64]']  # {'input_hash': ((58005.382189182565, [None, None], None), {}), 'hash': [3620.7217003386468, 54384.66048884392]}
    aten::t(t$58: bf16[576, 7168])  ->  t$59: bf16[7168, 576]  # {'input_hash': ((60000.702554143965,), {}), 'hash': 60000.702554143965}
    aten::view(t$34: bf16[1, 6, 7168], [6, 7168])  ->  t$60: bf16[6, 7168]  # {'input_hash': ((1156.63240952231, [None, None]), {}), 'hash': 1156.63240952231}
    aten::mm(t$60: bf16[6, 7168], t$59: bf16[7168, 576])  ->  t$61: bf16[6, 576]  # {'input_hash': ((1156.63240952231, 60000.702554143965), {}), 'hash': 925.7674539089203}
    aten::_unsafe_view(t$61: bf16[6, 576], [1, 6, 576])  ->  t$62: bf16[1, 6, 576]  # {'input_hash': ((925.7674539089203, [None, None, None]), {}), 'hash': 925.7674539089203}
    aten::split_with_sizes(t$62: bf16[1, 6, 576], [512, 64], -1)  ->  ['t$63: bf16[1, 6, 512]', 't$64: bf16[1, 6, 64]']  # {'input_hash': ((925.7674539089203, [None, None], None), {}), 'hash': [447.7636544704437, 478.00379943847656]}
    aten::_to_copy(t$63: bf16[1, 6, 512], dtype=torch.float32)  ->  t$65: f32[1, 6, 512]  # {'input_hash': ((447.7636544704437,), {'dtype': None}), 'hash': 447.7636544704437}
    aten::pow.Tensor_Scalar(t$65: f32[1, 6, 512], 2)  ->  t$66: f32[1, 6, 512]  # {'input_hash': ((447.7636544704437, None), {}), 'hash': 2416.9021114688285}
    aten::mean.dim(t$66: f32[1, 6, 512], [-1], True)  ->  t$67: f32[1, 6, 1]  # {'input_hash': ((2416.9021114688285, [None], None), {}), 'hash': 4.7205121368169785}
    aten::add.Tensor(t$67: f32[1, 6, 1], 1e-06)  ->  t$68: f32[1, 6, 1]  # {'input_hash': ((4.7205121368169785, None), {}), 'hash': 4.720518007874489}
    aten::rsqrt(t$68: f32[1, 6, 1])  ->  t$69: f32[1, 6, 1]  # {'input_hash': ((4.720518007874489,), {}), 'hash': 9.387228786945343}
    aten::mul.Tensor(t$65: f32[1, 6, 512], t$69: f32[1, 6, 1])  ->  t$70: f32[1, 6, 512]  # {'input_hash': ((447.7636544704437, 9.387228786945343), {}), 'hash': 681.568125214475}
    aten::_to_copy(t$70: f32[1, 6, 512], dtype=torch.bfloat16)  ->  t$71: bf16[1, 6, 512]  # {'input_hash': ((681.568125214475,), {'dtype': None}), 'hash': 681.4855184555054}
    aten::mul.Tensor(t$72: bf16[512], t$71: bf16[1, 6, 512])  ->  t$73: bf16[1, 6, 512]  # {'input_hash': ((3.804391235113144, 681.4855184555054), {}), 'hash': 5.108524536015466}
    aten::t(t$74: bf16[32768, 512])  ->  t$75: bf16[512, 32768]  # {'input_hash': ((54175.308287066175,), {}), 'hash': 54175.308287066175}
    aten::view(t$73: bf16[1, 6, 512], [6, 512])  ->  t$76: bf16[6, 512]  # {'input_hash': ((5.108524536015466, [None, None]), {}), 'hash': 5.108524536015466}
    aten::mm(t$76: bf16[6, 512], t$75: bf16[512, 32768])  ->  t$77: bf16[6, 32768]  # {'input_hash': ((5.108524536015466, 54175.308287066175), {}), 'hash': 111.4665225475328}
    aten::_unsafe_view(t$77: bf16[6, 32768], [1, 6, 32768])  ->  t$78: bf16[1, 6, 32768]  # {'input_hash': ((111.4665225475328, [None, None, None]), {}), 'hash': 111.4665225475328}
    aten::view(t$78: bf16[1, 6, 32768], [1, 6, -1, 256])  ->  t$79: bf16[1, 6, 128, 256]  # {'input_hash': ((111.4665225475328, [None, None, None, None]), {}), 'hash': 111.4665225475328}
    aten::transpose.int(t$79: bf16[1, 6, 128, 256], 1, 2)  ->  t$80: bf16[1, 128, 6, 256]  # {'input_hash': ((111.4665225475328, None, None), {}), 'hash': 111.4665225475328}
    aten::split_with_sizes(t$80: bf16[1, 128, 6, 256], [128, 128], -1)  ->  ['t$81: bf16[1, 128, 6, 128]', 't$82: bf16[1, 128, 6, 128]']  # {'input_hash': ((111.4665225475328, [None, None], None), {}), 'hash': [26.415770137362415, 85.05075241017039]}
    aten::view(t$64: bf16[1, 6, 64], [1, 1, 6, 64])  ->  t$83: bf16[1, 1, 6, 64]  # {'input_hash': ((478.00379943847656, [None, None, None, None]), {}), 'hash': 478.00379943847656}
    aten::unsqueeze(t$24: bf16[1, 6, 64], 1)  ->  t$84: bf16[1, 1, 6, 64]  # {'input_hash': ((339.74072265625, None), {}), 'hash': 339.74072265625}
    aten::unsqueeze(t$25: bf16[1, 6, 64], 1)  ->  t$85: bf16[1, 1, 6, 64]  # {'input_hash': ((76.23876954196976, None), {}), 'hash': 76.23876954196976}
    aten::view(t$57: bf16[1, 128, 6, 64], [1, 128, 6, 32, 2])  ->  t$86: bf16[1, 128, 6, 32, 2]  # {'input_hash': ((54384.66048884392, [None, None, None, None, None]), {}), 'hash': 54384.66048884392}
    aten::transpose.int(t$86: bf16[1, 128, 6, 32, 2], 4, 3)  ->  t$87: bf16[1, 128, 6, 2, 32]  # {'input_hash': ((54384.66048884392, None, None), {}), 'hash': 54384.66048884392}
    aten::clone(t$87: bf16[1, 128, 6, 2, 32], memory_format=torch.contiguous_format)  ->  t$88: bf16[1, 128, 6, 2, 32]  # {'input_hash': ((54384.66048884392,), {'memory_format': None}), 'hash': 54384.66048884392}
    aten::_unsafe_view(t$88: bf16[1, 128, 6, 2, 32], [1, 128, 6, 64])  ->  t$89: bf16[1, 128, 6, 64]  # {'input_hash': ((54384.66048884392, [None, None, None, None]), {}), 'hash': 54384.66048884392}
    aten::view(t$83: bf16[1, 1, 6, 64], [1, 1, 6, 32, 2])  ->  t$90: bf16[1, 1, 6, 32, 2]  # {'input_hash': ((478.00379943847656, [None, None, None, None, None]), {}), 'hash': 478.00379943847656}
    aten::transpose.int(t$90: bf16[1, 1, 6, 32, 2], 4, 3)  ->  t$91: bf16[1, 1, 6, 2, 32]  # {'input_hash': ((478.00379943847656, None, None), {}), 'hash': 478.00379943847656}
    aten::clone(t$91: bf16[1, 1, 6, 2, 32], memory_format=torch.contiguous_format)  ->  t$92: bf16[1, 1, 6, 2, 32]  # {'input_hash': ((478.00379943847656,), {'memory_format': None}), 'hash': 478.00379943847656}
    aten::_unsafe_view(t$92: bf16[1, 1, 6, 2, 32], [1, 1, 6, 64])  ->  t$93: bf16[1, 1, 6, 64]  # {'input_hash': ((478.00379943847656, [None, None, None, None]), {}), 'hash': 478.00379943847656}
    aten::mul.Tensor(t$89: bf16[1, 128, 6, 64], t$84: bf16[1, 1, 6, 64])  ->  t$94: bf16[1, 128, 6, 64]  # {'input_hash': ((54384.66048884392, 339.74072265625), {}), 'hash': 49226.70544719696}
    aten::slice.Tensor(t$89: bf16[1, 128, 6, 64], 3, 0, 32)  ->  t$95: bf16[1, 128, 6, 32]  # {'input_hash': ((54384.66048884392, None, None, None), {}), 'hash': 28109.381856679916}
    aten::slice.Tensor(t$89: bf16[1, 128, 6, 64], 3, 32, 9223372036854775807)  ->  t$96: bf16[1, 128, 6, 32]  # {'input_hash': ((54384.66048884392, None, None, None), {}), 'hash': 26275.278632164}
    aten::neg(t$96: bf16[1, 128, 6, 32])  ->  t$97: bf16[1, 128, 6, 32]  # {'input_hash': ((26275.278632164,), {}), 'hash': 26275.278632164}
    aten::cat(['t$97: bf16[1, 128, 6, 32]', 't$95: bf16[1, 128, 6, 32]'], -1)  ->  t$98: bf16[1, 128, 6, 64]  # {'input_hash': (([26275.278632164, 28109.381856679916], None), {}), 'hash': 54384.66048884392}
    aten::mul.Tensor(t$98: bf16[1, 128, 6, 64], t$85: bf16[1, 1, 6, 64])  ->  t$99: bf16[1, 128, 6, 64]  # {'input_hash': ((54384.66048884392, 76.23876954196976), {}), 'hash': 9243.289089380418}
    aten::add.Tensor(t$94: bf16[1, 128, 6, 64], t$99: bf16[1, 128, 6, 64])  ->  t$100: bf16[1, 128, 6, 64]  # {'input_hash': ((49226.70544719696, 9243.289089380418), {}), 'hash': 54235.34918093681}
    aten::mul.Tensor(t$93: bf16[1, 1, 6, 64], t$84: bf16[1, 1, 6, 64])  ->  t$101: bf16[1, 1, 6, 64]  # {'input_hash': ((478.00379943847656, 339.74072265625), {}), 'hash': 431.57656478881836}
    aten::slice.Tensor(t$93: bf16[1, 1, 6, 64], 3, 0, 32)  ->  t$102: bf16[1, 1, 6, 32]  # {'input_hash': ((478.00379943847656, None, None, None), {}), 'hash': 171.16246032714844}
    aten::slice.Tensor(t$93: bf16[1, 1, 6, 64], 3, 32, 9223372036854775807)  ->  t$103: bf16[1, 1, 6, 32]  # {'input_hash': ((478.00379943847656, None, None, None), {}), 'hash': 306.8413391113281}
    aten::neg(t$103: bf16[1, 1, 6, 32])  ->  t$104: bf16[1, 1, 6, 32]  # {'input_hash': ((306.8413391113281,), {}), 'hash': 306.8413391113281}
    aten::cat(['t$104: bf16[1, 1, 6, 32]', 't$102: bf16[1, 1, 6, 32]'], -1)  ->  t$105: bf16[1, 1, 6, 64]  # {'input_hash': (([306.8413391113281, 171.16246032714844], None), {}), 'hash': 478.00379943847656}
    aten::mul.Tensor(t$105: bf16[1, 1, 6, 64], t$85: bf16[1, 1, 6, 64])  ->  t$106: bf16[1, 1, 6, 64]  # {'input_hash': ((478.00379943847656, 76.23876954196976), {}), 'hash': 89.2572899106276}
    aten::add.Tensor(t$101: bf16[1, 1, 6, 64], t$106: bf16[1, 1, 6, 64])  ->  t$107: bf16[1, 1, 6, 64]  # {'input_hash': ((431.57656478881836, 89.2572899106276), {}), 'hash': 473.74784088134766}
    aten::expand(t$107: bf16[1, 1, 6, 64], [1, 128, 6, -1])  ->  t$108: bf16[1, 128, 6, 64]  # {'input_hash': ((473.74784088134766, [None, None, None, None]), {}), 'hash': 60639.7236328125}
    aten::cat(['t$56: bf16[1, 128, 6, 128]', 't$100: bf16[1, 128, 6, 64]'], -1)  ->  t$109: bf16[1, 128, 6, 192]  # {'input_hash': (([3620.7217003386468, 54235.34918093681], None), {}), 'hash': 57856.07088127546}
    aten::cat(['t$81: bf16[1, 128, 6, 128]', 't$108: bf16[1, 128, 6, 64]'], -1)  ->  t$110: bf16[1, 128, 6, 192]  # {'input_hash': (([26.415770137362415, 60639.7236328125], None), {}), 'hash': 60666.13940294986}
    aten::lift_fresh(t$111: bf16[0])  ->  t$111: bf16[0]  # {'input_hash': ((0.0,), {}), 'hash': 0.0}
    aten::lift_fresh(t$112: bf16[0])  ->  t$112: bf16[0]  # {'input_hash': ((0.0,), {}), 'hash': 0.0}
    aten::cat(['t$111: bf16[0]', 't$110: bf16[1, 128, 6, 192]'], -2)  ->  t$113: bf16[1, 128, 6, 192]  # {'input_hash': (([0.0, 60666.13940294986], None), {}), 'hash': 60666.13940294986}
    aten::cat(['t$112: bf16[0]', 't$82: bf16[1, 128, 6, 128]'], -2)  ->  t$114: bf16[1, 128, 6, 128]  # {'input_hash': (([0.0, 85.05075241017039], None), {}), 'hash': 85.05075241017039}
    aten::_to_copy(t$109: bf16[1, 128, 6, 192], dtype=torch.float32)  ->  t$115: f32[1, 128, 6, 192]  # {'input_hash': ((57856.07088127546,), {'dtype': None}), 'hash': 57856.07088127546}
    aten::_to_copy(t$113: bf16[1, 128, 6, 192], dtype=torch.float32)  ->  t$116: f32[1, 128, 6, 192]  # {'input_hash': ((60666.13940294986,), {'dtype': None}), 'hash': 60666.13940294986}
    aten::_to_copy(t$114: bf16[1, 128, 6, 128], dtype=torch.float32)  ->  t$117: f32[1, 128, 6, 128]  # {'input_hash': ((85.05075241017039,), {'dtype': None}), 'hash': 85.05075241017039}
    aten::mul.Scalar(t$115: f32[1, 128, 6, 192], 0.3677414565436974)  ->  t$118: f32[1, 128, 6, 192]  # {'input_hash': ((57856.07088127546, None), {}), 'hash': 21276.07629972101}
    aten::ones([6, 6], dtype=torch.bool, layout=torch.strided, device=cpu)  ->  t$119: b8[6, 6]  # {'hash': 36.0}
    aten::tril(t$119: b8[6, 6])  ->  t$120: b8[6, 6]  # {'input_hash': ((36.0,), {}), 'hash': 21.0}
    aten::scalar_tensor(-inf, dtype=torch.float32, device=cpu)  ->  t$121: f32[]  # {'hash': inf}
    aten::scalar_tensor(0.0, dtype=torch.float32, layout=torch.strided, device=cpu)  ->  t$122: f32[]  # {'hash': 0.0}
    aten::where.self(t$120: b8[6, 6], t$122: f32[], t$121: f32[])  ->  t$123: f32[6, 6]  # {'input_hash': ((21.0, 0.0, inf), {}), 'hash': inf}
    aten::transpose.int(t$116: f32[1, 128, 6, 192], -2, -1)  ->  t$124: f32[1, 128, 192, 6]  # {'input_hash': ((60666.13940294986, None, None), {}), 'hash': 60666.13940294986}
    aten::mul.Scalar(t$124: f32[1, 128, 192, 6], 0.3677414565436974)  ->  t$125: f32[1, 128, 192, 6]  # {'input_hash': ((60666.13940294986, None), {}), 'hash': 22309.455075962735}
    aten::expand(t$118: f32[1, 128, 6, 192], [1, 128, 6, 192])  ->  t$126: f32[1, 128, 6, 192]  # {'input_hash': ((21276.07629972101, [None, None, None, None]), {}), 'hash': 21276.07629972101}
    aten::view(t$126: f32[1, 128, 6, 192], [128, 6, 192])  ->  t$127: f32[128, 6, 192]  # {'input_hash': ((21276.07629972101, [None, None, None]), {}), 'hash': 21276.07629972101}
    aten::expand(t$125: f32[1, 128, 192, 6], [1, 128, 192, 6])  ->  t$128: f32[1, 128, 192, 6]  # {'input_hash': ((22309.455075962735, [None, None, None, None]), {}), 'hash': 22309.455075962735}
    aten::view(t$128: f32[1, 128, 192, 6], [128, 192, 6])  ->  t$129: f32[128, 192, 6]  # {'input_hash': ((22309.455075962735, [None, None, None]), {}), 'hash': 22309.455075962735}
    aten::bmm(t$127: f32[128, 6, 192], t$129: f32[128, 192, 6])  ->  t$130: f32[128, 6, 6]  # {'input_hash': ((21276.07629972101, 22309.455075962735), {}), 'hash': 20362.785286933184}
    aten::_unsafe_view(t$130: f32[128, 6, 6], [1, 128, 6, 6])  ->  t$131: f32[1, 128, 6, 6]  # {'input_hash': ((20362.785286933184, [None, None, None, None]), {}), 'hash': 20362.785286933184}
    aten::add.Tensor(t$131: f32[1, 128, 6, 6], t$123: f32[6, 6])  ->  t$132: f32[1, 128, 6, 6]  # {'input_hash': ((20362.785286933184, inf), {}), 'hash': inf}
    aten::_safe_softmax(t$132: f32[1, 128, 6, 6], -1)  ->  t$133: f32[1, 128, 6, 6]  # {'input_hash': ((inf, None), {}), 'hash': 767.9999987812288}
    aten::_to_copy(t$133: f32[1, 128, 6, 6], dtype=torch.bfloat16)  ->  t$134: bf16[1, 128, 6, 6]  # {'input_hash': ((767.9999987812288,), {'dtype': None}), 'hash': 767.9952215677288}
    aten::expand(t$133: f32[1, 128, 6, 6], [1, 128, 6, 6])  ->  t$135: f32[1, 128, 6, 6]  # {'input_hash': ((767.9999987812288, [None, None, None, None]), {}), 'hash': 767.9999987812288}
    aten::view(t$135: f32[1, 128, 6, 6], [128, 6, 6])  ->  t$136: f32[128, 6, 6]  # {'input_hash': ((767.9999987812288, [None, None, None]), {}), 'hash': 767.9999987812288}
    aten::expand(t$117: f32[1, 128, 6, 128], [1, 128, 6, 128])  ->  t$137: f32[1, 128, 6, 128]  # {'input_hash': ((85.05075241017039, [None, None, None, None]), {}), 'hash': 85.05075241017039}
    aten::view(t$137: f32[1, 128, 6, 128], [128, 6, 128])  ->  t$138: f32[128, 6, 128]  # {'input_hash': ((85.05075241017039, [None, None, None]), {}), 'hash': 85.05075241017039}
    aten::bmm(t$136: f32[128, 6, 6], t$138: f32[128, 6, 128])  ->  t$139: f32[128, 6, 128]  # {'input_hash': ((767.9999987812288, 85.05075241017039), {}), 'hash': 64.34864976890512}
    aten::_unsafe_view(t$139: f32[128, 6, 128], [1, 128, 6, 128])  ->  t$140: f32[1, 128, 6, 128]  # {'input_hash': ((64.34864976890512, [None, None, None, None]), {}), 'hash': 64.34864976890512}
    aten::_to_copy(t$140: f32[1, 128, 6, 128], dtype=torch.bfloat16)  ->  t$141: bf16[1, 128, 6, 128]  # {'input_hash': ((64.34864976890512,), {'dtype': None}), 'hash': 64.35423506208463}
    aten::transpose.int(t$141: bf16[1, 128, 6, 128], 1, 2)  ->  t$142: bf16[1, 6, 128, 128]  # {'input_hash': ((64.35423506208463, None, None), {}), 'hash': 64.35423506208463}
    aten::clone(t$142: bf16[1, 6, 128, 128], memory_format=torch.contiguous_format)  ->  t$143: bf16[1, 6, 128, 128]  # {'input_hash': ((64.35423506208463,), {'memory_format': None}), 'hash': 64.35423506208463}
    aten::view(t$143: bf16[1, 6, 128, 128], [1, 6, -1])  ->  t$144: bf16[1, 6, 16384]  # {'input_hash': ((64.35423506208463, [None, None, None]), {}), 'hash': 64.35423506208463}
    aten::t(t$145: bf16[7168, 16384])  ->  t$146: bf16[16384, 7168]  # {'input_hash': ((402437.33954404993,), {}), 'hash': 402437.33954404993}
    aten::view(t$144: bf16[1, 6, 16384], [6, 16384])  ->  t$147: bf16[6, 16384]  # {'input_hash': ((64.35423506208463, [None, None]), {}), 'hash': 64.35423506208463}
    aten::mm(t$147: bf16[6, 16384], t$146: bf16[16384, 7168])  ->  t$148: bf16[6, 7168]  # {'input_hash': ((64.35423506208463, 402437.33954404993), {}), 'hash': 236.73609862662852}
    aten::_unsafe_view(t$148: bf16[6, 7168], [1, 6, 7168])  ->  t$149: bf16[1, 6, 7168]  # {'input_hash': ((236.73609862662852, [None, None, None]), {}), 'hash': 236.73609862662852}
    aten::add.Tensor(t$2: bf16[1, 6, 7168], t$149: bf16[1, 6, 7168])  ->  t$150: bf16[1, 6, 7168]  # {'input_hash': ((788.8026814290788, 236.73609862662852), {}), 'hash': 831.3630962371826}
    aten::_to_copy(t$150: bf16[1, 6, 7168], dtype=torch.float32)  ->  t$151: f32[1, 6, 7168]  # {'input_hash': ((831.3630962371826,), {'dtype': None}), 'hash': 831.3630962371826}
    aten::pow.Tensor_Scalar(t$151: f32[1, 6, 7168], 2)  ->  t$152: f32[1, 6, 7168]  # {'input_hash': ((831.3630962371826, None), {}), 'hash': 58.23028035571633}
    aten::mean.dim(t$152: f32[1, 6, 7168], [-1], True)  ->  t$153: f32[1, 6, 1]  # {'input_hash': ((58.23028035571633, [None], None), {}), 'hash': 0.008123643870931119}
    aten::add.Tensor(t$153: f32[1, 6, 1], 1e-06)  ->  t$154: f32[1, 6, 1]  # {'input_hash': ((0.008123643870931119, None), {}), 'hash': 0.008129643916618079}
    aten::rsqrt(t$154: f32[1, 6, 1])  ->  t$155: f32[1, 6, 1]  # {'input_hash': ((0.008129643916618079,), {}), 'hash': 169.5744113922119}
    aten::mul.Tensor(t$151: f32[1, 6, 7168], t$155: f32[1, 6, 1])  ->  t$156: f32[1, 6, 7168]  # {'input_hash': ((831.3630962371826, 169.5744113922119), {}), 'hash': 22519.253141999594}
    aten::_to_copy(t$156: f32[1, 6, 7168], dtype=torch.bfloat16)  ->  t$157: bf16[1, 6, 7168]  # {'input_hash': ((22519.253141999594,), {'dtype': None}), 'hash': 22520.068055152893}
    aten::mul.Tensor(t$158: bf16[7168], t$157: bf16[1, 6, 7168])  ->  t$159: bf16[1, 6, 7168]  # {'input_hash': ((122.01806432008743, 22520.068055152893), {}), 'hash': 602.6456905032974}
    aten::t(t$160: bf16[18432, 7168])  ->  t$161: bf16[7168, 18432]  # {'input_hash': ((101450.95126219967,), {}), 'hash': 101450.95126219967}
    aten::view(t$159: bf16[1, 6, 7168], [6, 7168])  ->  t$162: bf16[6, 7168]  # {'input_hash': ((602.6456905032974, [None, None]), {}), 'hash': 602.6456905032974}
    aten::mm(t$162: bf16[6, 7168], t$161: bf16[7168, 18432])  ->  t$163: bf16[6, 18432]  # {'input_hash': ((602.6456905032974, 101450.95126219967), {}), 'hash': 780578.6303224564}
    aten::_unsafe_view(t$163: bf16[6, 18432], [1, 6, 18432])  ->  t$164: bf16[1, 6, 18432]  # {'input_hash': ((780578.6303224564, [None, None, None]), {}), 'hash': 780578.6303224564}
    aten::silu(t$164: bf16[1, 6, 18432])  ->  t$165: bf16[1, 6, 18432]  # {'input_hash': ((780578.6303224564,), {}), 'hash': 706.0014340877533}
    aten::t(t$166: bf16[18432, 7168])  ->  t$167: bf16[7168, 18432]  # {'input_hash': ((301350.00236657704,), {}), 'hash': 301350.00236657704}
    aten::view(t$159: bf16[1, 6, 7168], [6, 7168])  ->  t$168: bf16[6, 7168]  # {'input_hash': ((602.6456905032974, [None, None]), {}), 'hash': 602.6456905032974}
    aten::mm(t$168: bf16[6, 7168], t$167: bf16[7168, 18432])  ->  t$169: bf16[6, 18432]  # {'input_hash': ((602.6456905032974, 301350.00236657704), {}), 'hash': 2254.3726884126663}
    aten::_unsafe_view(t$169: bf16[6, 18432], [1, 6, 18432])  ->  t$170: bf16[1, 6, 18432]  # {'input_hash': ((2254.3726884126663, [None, None, None]), {}), 'hash': 2254.3726884126663}
    aten::mul.Tensor(t$165: bf16[1, 6, 18432], t$170: bf16[1, 6, 18432])  ->  t$171: bf16[1, 6, 18432]  # {'input_hash': ((706.0014340877533, 2254.3726884126663), {}), 'hash': 20.34076667185036}
    aten::t(t$172: bf16[7168, 18432])  ->  t$173: bf16[18432, 7168]  # {'input_hash': ((284068.9295961037,), {}), 'hash': 284068.9295961037}
    aten::view(t$171: bf16[1, 6, 18432], [6, 18432])  ->  t$174: bf16[6, 18432]  # {'input_hash': ((20.34076667185036, [None, None]), {}), 'hash': 20.34076667185036}
    aten::mm(t$174: bf16[6, 18432], t$173: bf16[18432, 7168])  ->  t$175: bf16[6, 7168]  # {'input_hash': ((20.34076667185036, 284068.9295961037), {}), 'hash': 268.8421404538676}
    aten::_unsafe_view(t$175: bf16[6, 7168], [1, 6, 7168])  ->  t$176: bf16[1, 6, 7168]  # {'input_hash': ((268.8421404538676, [None, None, None]), {}), 'hash': 268.8421404538676}
    aten::add.Tensor(t$150: bf16[1, 6, 7168], t$176: bf16[1, 6, 7168])  ->  t$177: bf16[1, 6, 7168]  # {'input_hash': ((831.3630962371826, 268.8421404538676), {}), 'hash': 872.0488355457783}
    aten::_to_copy(t$177: bf16[1, 6, 7168], dtype=torch.float32)  ->  t$178: f32[1, 6, 7168]  # {'input_hash': ((872.0488355457783,), {'dtype': None}), 'hash': 872.0488355457783}
    aten::pow.Tensor_Scalar(t$178: f32[1, 6, 7168], 2)  ->  t$179: f32[1, 6, 7168]  # {'input_hash': ((872.0488355457783, None), {}), 'hash': 62.40043227744725}
    aten::mean.dim(t$179: f32[1, 6, 7168], [-1], True)  ->  t$180: f32[1, 6, 1]  # {'input_hash': ((62.40043227744725, [None], None), {}), 'hash': 0.008705417509190738}
    aten::add.Tensor(t$180: f32[1, 6, 1], 1e-06)  ->  t$181: f32[1, 6, 1]  # {'input_hash': ((0.008705417509190738, None), {}), 'hash': 0.008711417554877698}
    aten::rsqrt(t$181: f32[1, 6, 1])  ->  t$182: f32[1, 6, 1]  # {'input_hash': ((0.008711417554877698,), {}), 'hash': 163.1077537536621}
    aten::mul.Tensor(t$178: f32[1, 6, 7168], t$182: f32[1, 6, 1])  ->  t$183: f32[1, 6, 7168]  # {'input_hash': ((872.0488355457783, 163.1077537536621), {}), 'hash': 22274.217669785372}
    aten::_to_copy(t$183: f32[1, 6, 7168], dtype=torch.bfloat16)  ->  t$184: bf16[1, 6, 7168]  # {'input_hash': ((22274.217669785372,), {'dtype': None}), 'hash': 22274.649938583374}
    aten::mul.Tensor(t$185: bf16[7168], t$184: bf16[1, 6, 7168])  ->  t$186: bf16[1, 6, 7168]  # {'input_hash': ((2747.7391052246094, 22274.649938583374), {}), 'hash': 8408.873062133789}
    aten::alias(t$186: bf16[1, 6, 7168])  ->  t$187: bf16[1, 6, 7168]  # {'input_hash': ((8408.873062133789,), {}), 'hash': 8408.873062133789}
    aten::t(t$188: bf16[129280, 7168])  ->  t$189: bf16[7168, 129280]  # {'input_hash': ((58439660.51988735,), {}), 'hash': 58439660.51988734}
    aten::view(t$187: bf16[1, 6, 7168], [6, 7168])  ->  t$190: bf16[6, 7168]  # {'input_hash': ((8408.873062133789, [None, None]), {}), 'hash': 8408.873062133789}
    aten::mm(t$190: bf16[6, 7168], t$189: bf16[7168, 129280])  ->  t$191: bf16[6, 129280]  # {'input_hash': ((8408.873062133789, 58439660.51988734), {}), 'hash': 1421614.0652765036}
    aten::_unsafe_view(t$191: bf16[6, 129280], [1, 6, 129280])  ->  t$192: bf16[1, 6, 129280]  # {'input_hash': ((1421614.0652765036, [None, None, None]), {}), 'hash': 1421614.0652765036}
CausalLMOutputWithPast(loss=None, logits=tensor([[[ 9.6875e+00, -4.0938e+00,  8.0469e-01,  ...,  1.0312e+00,
           8.4766e-01,  7.7734e-01],
         [ 2.4707e-01, -1.8125e+00, -1.4551e-01,  ...,  1.2695e-01,
           4.2419e-03,  2.6733e-02],
         [ 1.2734e+00, -2.2969e+00,  4.1016e-01,  ...,  3.2422e-01,
           3.0664e-01,  4.7852e-01],
         [ 5.1250e+00, -6.3438e+00,  2.1973e-01,  ...,  2.7734e-01,
           3.2227e-01,  1.6895e-01],
         [-1.0469e+00, -2.1250e+00,  4.7266e-01,  ...,  8.1543e-02,
           9.8145e-02,  6.3281e-01],
         [ 2.3438e-01, -8.7402e-02,  3.1055e-01,  ...,  2.0117e-01,
           1.8848e-01,  4.2383e-01]]], dtype=torch.bfloat16), past_key_values=DynamicCache(layers=[DynamicLayer]), hidden_states=None, attentions=None)
```

</td>
</tr>
</table>


The 3rd line is diff! Let check the stack trace for more details.
```bash
    ...

    # File: /mnt/disk3/yiliu4/t47/lib/python3.12/site-packages/transformers/models/deepseek_v3/modeling_deepseek_v3.py:571 in forward, code: position_ids = cache_position.unsqueeze(0)
    aten::unsqueeze(t$3: i64[6], 0)  ->  t$4: i64[1, 6]  # {'input_hash': ((15.0, None), {}), 'hash': 15.0}

    # File: /mnt/disk3/yiliu4/t47/lib/python3.12/site-packages/transformers/models/deepseek_v3/modeling_deepseek_v3.py:79 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
    aten::unsqueeze(t$5: f32[32], 0)  ->  t$6: f32[1, 32]  # {'input_hash': ((3.9489362656597677, None), {}), 'hash': 3.9489362656597677}
```

```bash
    ...

    # File: /mnt/disk3/yiliu4/transformers/src/transformers/models/deepseek_v3/modeling_deepseek_v3.py:623 in forward, code: position_ids = cache_position.unsqueeze(0)
    aten::unsqueeze(t$4: i64[6], 0)  ->  t$5: i64[1, 6]  # {'input_hash': ((15.0, None), {}), 'hash': 15.0}

    # File: /mnt/disk3/yiliu4/transformers/src/transformers/models/deepseek_v3/modeling_deepseek_v3.py:109 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
    aten::unsqueeze(t$6: f32[32], 0)  ->  t$7: f32[1, 32]  # {'input_hash': ((9.351147240759962e+31, None), {}), 'hash': 9.351147240759962e+31}
```
So the input of `aten::unsqueeze` is diff, let's check the source code.
```python
    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device) #<----------
        position_ids_expanded = position_ids[:, None, :].float()
    ...

```
And we can find the `self.inv_freq` is diff, after check the code, the `self.inv_freq` was initialized at `DeepseekV3RotaryEmbedding` `__init__` stage. But in v5, the model was initialized with `meta` device, so we need to post-porcess that at `_init_weights` stage. But these lines was commented me due to it causes too much time.......
Any way the probloem was solved by add the `_init_weights` back.
 
```python
@auto_docstring
class DeepseekV3PreTrainedModel(PreTrainedModel):
    config: DeepseekV3Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["DeepseekV3DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = (
        is_grouped_mm_available()
    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": DeepseekV3DecoderLayer,
        "attentions": DeepseekV3Attention,
    }
    _keep_in_fp32_modules_strict = ["e_score_correction_bias"]

    @torch.no_grad()
    def _init_weights(self, module):
        pass
        # super()._init_weights(module)
        # if isinstance(module, DeepseekV3TopkRouter):
        #     init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
        #     init.zeros_(module.e_score_correction_bias)
        # elif isinstance(module, DeepseekV3NaiveMoe):
        #     init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)
        #     init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)
        # if "RotaryEmbedding" in module.__class__.__name__ and hasattr(module, "original_inv_freq"):
        #     rope_fn = (
        #         ROPE_INIT_FUNCTIONS[module.rope_type]
        #         if module.rope_type != "default"
        #         else module.compute_default_rope_parameters
        #     )
        #     buffer_value, _ = rope_fn(module.config)
        #     init.copy_(module.inv_freq, buffer_value)
        #     init.copy_(module.original_inv_freq, buffer_value)
```



### Introduce torch `DebugMode`

The `DebugMode` is inherit from the `TorchDispatchMode`, and inject the torch op calls(`__torch_function__` or `__torch_dispatch__`) to record the input/output of each call. More details can be found in [Torch docs](https://docs.pytorch.org/tutorials/recipes/debug_mode_tutorial.html).

Below is a demo copied from docs:
```python
from torch._inductor.decomposition import decomps_to_exclude
import torch
from torch.utils._debug_mode import DebugMode

def run_model(model, data, *, compile_with=None):
    if compile_with is not None:
        model = torch.compile(model, backend=compile_with)
    with DebugMode(record_output=True) as dm, DebugMode.log_tensor_hashes(
        hash_inputs=True,
    ):
        dm_out = model(*data)
    return dm, dm_out

class Toy(torch.nn.Module):
    def forward(self, x):
        return torch.relu(x).mm(x.T)

inputs = (torch.randn(4, 4),)
dm_eager, _ = run_model(Toy(), inputs)
print("Eager mode:")
print(dm_eager.debug_string())

# Output:
# Eager mode:
#     aten::relu(t: f32[4, 4])  ->  t: f32[4, 4]  # {'input_hash': ((14.893587063997984,), {}), 'hash': 7.259045481681824}
#     aten::permute(t: f32[4, 4], [1, 0])  ->  t: f32[4, 4]  # {'input_hash': ((14.893587063997984, [None, None]), {}), 'hash': 14.893587063997984}
#     aten::mm(t: f32[4, 4], t: f32[4, 4])  ->  t: f32[4, 4]  # {'input_hash': ((7.259045481681824, 14.893587063997984), {}), 'hash': 26.860059764236212}
```


## Environment

| Component | v4.57.3 | v5.0 |
|-----------|---------|------|
| Transformers | 4.57.3 | 5.0.0 |
| PyTorch | [version] | [version] |
| Model | [model name] | [model name] |
| Device | [device] | [device] |

## Debug Setup

### PyTorch Debug Mode Configuration

```python
# Enable debug mode for layer-wise comparison
import torch
torch.autograd.set_detect_anomaly(True)

# Your debug setup code here
```

### Comparison Methodology

- **Approach:** [Describe your comparison approach]
- **Layers Monitored:** [List specific layers]
- **Metrics Used:** [Absolute diff, relative diff, etc.]

---

## Layer-by-Layer Comparison

### Layer 0: Embedding Layer

#### Side-by-Side Comparison

| v4.57.3 | v5.0 |
|---------|------|
| **Output Shape:** `[batch, seq_len, hidden_dim]` | **Output Shape:** `[batch, seq_len, hidden_dim]` |
| **Mean:** 0.0234 | **Mean:** 0.0234 |
| **Std:** 1.0045 | **Std:** 1.0045 |
| **Min:** -3.456 | **Min:** -3.456 |
| **Max:** 3.789 | **Max:** 3.789 |

**Difference:**
- Max absolute diff: `0.0001`
- Mean absolute diff: `0.00001`
- Status: âœ… PASS

#### Top-Down Comparison

**v4.57.3 Output:**
```
tensor([[ 0.1234,  0.5678, -0.9012, ...],
        [-0.3456,  0.7890,  0.1234, ...],
        ...])
```

**v5.0 Output:**
```
tensor([[ 0.1234,  0.5678, -0.9012, ...],
        [-0.3456,  0.7890,  0.1234, ...],
        ...])
```

**Analysis:**
- [Your analysis here]

---

### Layer 1: Attention Layer

#### Side-by-Side Comparison

| Metric | v4.57.3 | v5.0 | Diff |
|--------|---------|------|------|
| **Q Output Mean** | [value] | [value] | [diff] |
| **K Output Mean** | [value] | [value] | [diff] |
| **V Output Mean** | [value] | [value] | [diff] |
| **Attention Weights Mean** | [value] | [value] | [diff] |
| **Output Mean** | [value] | [value] | [diff] |

**Status:** [âœ… PASS / âš ï¸ WARNING / âŒ FAIL]

#### Detailed Output Comparison

<details>
<summary>Click to expand full outputs</summary>

**v4.57.3:**
```python
# Attention output
tensor([[...]])
```

**v5.0:**
```python
# Attention output
tensor([[...]])
```

</details>

---

### Layer 2: Feed-Forward Network

#### Stacked Comparison

**v4.57.3 FFN Intermediate:**
```
Shape: [batch, seq_len, ffn_dim]
Mean: [value], Std: [value]
Sample values: [...]
```

**v5.0 FFN Intermediate:**
```
Shape: [batch, seq_len, ffn_dim]
Mean: [value], Std: [value]
Sample values: [...]
```

**Difference Analysis:**
- Absolute diff: [value]
- Relative diff: [value]%
- Divergence points: [describe if any]

---

## Critical Differences Found

### Issue 1: [Issue Title]

**Location:** Layer [X], Component [Y]

| Aspect | v4.57.3 | v5.0 |
|--------|---------|------|
| Behavior | [description] | [description] |
| Output Range | [range] | [range] |
| Impact | - | [impact description] |

**Code Comparison:**

<table>
<tr>
<th>v4.57.3</th>
<th>v5.0</th>
</tr>
<tr>
<td>

```python
# Old implementation
def old_method():
    # code here
    pass
```

</td>
<td>

```python
# New implementation
def new_method():
    # code here
    pass
```

</td>
</tr>
</table>

**Root Cause:** [Explanation]

**Solution:** [Proposed fix or workaround]

---

### Issue 2: [Issue Title]

[Same structure as Issue 1]

---

## Statistical Summary

### Aggregate Differences by Layer Type

| Layer Type | Mean Abs Diff | Max Abs Diff | Relative Diff % | Status |
|------------|---------------|--------------|-----------------|--------|
| Embedding | [value] | [value] | [value] | âœ… |
| Attention | [value] | [value] | [value] | âš ï¸ |
| FFN | [value] | [value] | [value] | âœ… |
| LayerNorm | [value] | [value] | [value] | âœ… |
| Output | [value] | [value] | [value] | âŒ |

---

## Visualization

### Difference Heatmap

```
[Insert visualization or ASCII representation]
```

### Per-Layer Divergence Plot

```
Layer 0:  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ (10% diff)
Layer 1:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ (90% diff) âš ï¸
Layer 2:  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ (15% diff)
Layer 3:  â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ (8% diff)
...
```

---

## Debugging Tools & Commands

### Extract Layer Outputs

```python
# Hook to capture layer outputs
def register_hooks(model):
    outputs = {}
    def hook_fn(name):
        def hook(module, input, output):
            outputs[name] = output.detach().clone()
        return hook
    
    for name, layer in model.named_modules():
        layer.register_forward_hook(hook_fn(name))
    
    return outputs
```

### Compare Outputs

```python
# Comparison function
def compare_outputs(out1, out2, threshold=1e-5):
    abs_diff = torch.abs(out1 - out2)
    max_diff = abs_diff.max().item()
    mean_diff = abs_diff.mean().item()
    rel_diff = (abs_diff / (torch.abs(out1) + 1e-10)).mean().item()
    
    return {
        'max_abs_diff': max_diff,
        'mean_abs_diff': mean_diff,
        'rel_diff': rel_diff,
        'pass': max_diff < threshold
    }
```

---

## Action Items

- [ ] Investigate attention layer difference in Layer 1
- [ ] Compare positional encoding implementation
- [ ] Check activation function changes
- [ ] Verify normalization behavior
- [ ] Test with different precision (fp16/fp32/bf16)
- [ ] Profile performance differences

---

## Lessons Learned

1. **[Lesson 1]:** [Description]
2. **[Lesson 2]:** [Description]
3. **[Lesson 3]:** [Description]

---

## References

- [Transformers v5.0 Release Notes](https://github.com/huggingface/transformers/releases)
- [Migration Guide](https://huggingface.co/docs/transformers/migration)
- [PyTorch Debug Mode Documentation](https://pytorch.org/docs/stable/autograd.html)

---

## Appendix

### Full Configuration Files

<details>
<summary>config_v4.json</summary>

```json
{
  "config": "here"
}
```

</details>

<details>
<summary>config_v5.json</summary>

```json
{
  "config": "here"
}
```

</details>

### Test Cases

```python
# Add your test cases here
```

---

**Last Updated:** January 24, 2026
